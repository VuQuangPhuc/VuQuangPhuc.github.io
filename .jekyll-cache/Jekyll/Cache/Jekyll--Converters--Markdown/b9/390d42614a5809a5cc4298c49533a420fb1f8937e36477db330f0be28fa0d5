I"ò<h2 id="introduction">Introduction</h2>

<p>Simulations are a large part of scientific computing.
In fact, when looking application of the <a href="https://en.wikipedia.org/wiki/TOP500">largest supercomputers</a>, most of them are being used for the purpose of simulation in one way or another.
Simulations of cells, nuclear reactions or even the spread of the corona virus are just some of the applications.</p>

<p>Physics simulations are one sub-section of these.
Researchers are interested in how objects with complex physics interact with each other.
Most of these simulations are based on numercial solvers that use some kind of physics model.</p>

<p>A new approch is to train Graph Networks to simulate complex physics simulations.
This blog will explore why this approach is so suitable for this type as simulation as well as introduce a framework for using Graph Networks as described by DeepMind [1].</p>

<h2 id="why-use-deep-learning-at-all">Why use deep learning at all?</h2>

<p>To understand why we want to learn a simulation at all, we have to look at the drawbacks of traditional engineered simulators.</p>

<ol>
  <li>Performance: 
Engineered simulators are expensive to run as they rely on huge amounts of calculations based on their physics models in order to produce high-quality results in high resolution.
This requires the usage of 
Learned simulators are able to learn the subgrid dynamics.</li>
</ol>

<p>Another part is the rise of purpose-build accelerator hardware for AI-based systems, which allows them to run more efficiently.</p>
<ol>
  <li>Learn unknown physics: Engineered simulations rely on their physics model. This model requires full knowledge, which in most cases is not available. Learned simulations can compensate by learning unknown physics.</li>
  <li>Generalizability: Simulations can often be very specialized, Graph Networks can be build to scale to different use cases.</li>
</ol>

<h2 id="graph-networks">Graph Networks</h2>

<p><img src="/assets/graph_ex.png" alt="" /></p>

<p>A Graph Network (GN) is a type of Graph Neural Network that maps an input graphto an output graph with the same structure but potentially different node, edge, andgraph-level attributes.
This is in contrast other types of neural networks.
Convolutional Neural Networks take inputs in form of grids.
Multilayer perceptrons take inputs in form of sequences.</p>

<p>Graphs are, although also a structured data type, by nature less structured as they can take arbitrary shapes and sizes.</p>

<p>This works well for physics based simulations, in particular particle based simulations as particles can be directly mapped to nodes, which are connected via edges.</p>

<h2 id="message-passing">Message passing</h2>

<p>The question is now is: how do we use these graph networks to learn?
Most standard machine learning models rely on the assumption that data is independent and identically distributed.
This is not the case with graphs, where nodes are interconnected.</p>

<p>Instead models were created that directly operate on graphs and that are able to make use of graph properties.
The model used in DeepMinds paper is based on the general architecture of a Message Passing Neural Network (MPNN).</p>

<p><img src="/assets/graph_1.png" alt="" /></p>

<p>The idea behind message passing is realtively simple.</p>

<p>First we are given a Graph \(G = (u, \{v_i\}_{i = 1 \dots N_n}, \{e_j, s_j, r_j\}_{j = 1 \dots N_e})\).
u is a vector of global features, \(\{v_i\}_{i = 1 \dots N_n}\) is a nodes where each \(v_i\) is a vector of node features and \(\{e_j, s_j, r_j\}_{j = 1 \dots N_e}\) is a set of directed edges where \(e_j\) is a vector of edge features, with \(s_j\) and \(r_j\) being the indices of the sender and receiver nodes.</p>

<p>The general message passing as presented by Gilmer et. al. [2] consists of three phases.
The first phase constructs messages for all nodes.</p>

\[m_{i}' = \sum_{w \in N(i)} M(v_i, v_w, e_{iw})\]

<p>\(M\) is the message function that takes the two nodes and the edge connecting these nodes as an input.
A message \(m_i\) for the node \(i\) is an aggregation over all message function for all nodes in the neighborhood of node \(i\)</p>

\[v_{i}'\]

<p>They are, in combination with the old states, the input of a function that determines the updated values of the feature vectors.
These two phases combine to be one message passing step and can be repeated an arbitrary amount of times before moving on to the third phase, that phase being the readout phase.
Here we use the final state of the latent vectors as an input for another function that determines the final output of the graph network.</p>

<p>##</p>

<h2 id="references">References</h2>

<p>[1] Sanchez-Gonzalez, A., Heess, N., Springenberg, J. T., Merel, J., Riedmiller, M.,Hadsell, R., &amp; Battaglia, P. (2018). Graph networks as learnable physicsengines for inference and control. InInternational conference on machinelearning. PMLR.
[2] Gilmer, J., Schoenholz, S., Riley, P., Vinyals, O., &amp; Dahl, G.. (2017). Neural Message Passing for Quantum Chemistry.</p>
:ET